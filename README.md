<h1 align="center">Higgs Audio V2: Vietnamese TTS Training Guide</h1>

<div align="center" style="display: flex; justify-content: center; margin-top: 10px;">
  <a href="https://boson.ai/blog/higgs-audio-v2"><img src='https://img.shields.io/badge/ğŸš€-Launch Blogpost-228B22' style="margin-right: 5px;"></a>
  <a href="https://boson.ai/demo/tts"><img src="https://img.shields.io/badge/ğŸ•¹ï¸-Boson%20AI%20Playground-9C276A" style="margin-right: 5px;"></a>
  <a href="https://huggingface.co/spaces/smola/higgs_audio_v2"><img src="https://img.shields.io/badge/ğŸ®-HF%20Space%20Playground-8A2BE2" style="margin-right: 5px;"></a>
  <a href="https://huggingface.co/bosonai/higgs-audio-v2-generation-3B-base"><img src="https://img.shields.io/badge/ğŸ¤—-Checkpoints (3.6B LLM + 2.2B audio adapter)-ED5A22.svg" style="margin-right: 5px;"></a>
</div>

# Vietnamese TTS Training Repository for Higgs Audio v2

This repository provides a complete training pipeline for fine-tuning Higgs Audio v2 specifically for Vietnamese Text-to-Speech (TTS) tasks. The setup includes optimized data processing, training scripts, and comprehensive documentation for Vietnamese language support.

## ï¿½ Features

- **ğŸ‡»ğŸ‡³ Vietnamese Language Support**: Specialized for Vietnamese TTS with proper emotion/scene detection
- **âš¡ High-Performance Processing**: 70% faster data preprocessing with multithreading
- **ğŸ§  Memory Efficient Training**: LoRA fine-tuning **requires 24GB+ GPU memory** (see analysis below)
- **ğŸ“Š Comprehensive Dataset**: 38,625 Vietnamese samples (42.4 hours) from 91 speakers
- **ğŸ”§ One-Click Training**: Automated scripts for easy setup and training
- **ğŸ“ˆ Training Monitoring**: TensorBoard integration for progress tracking

## ï¿½ğŸ¥ Tutorial Videos

| Platform | Link |
|----------|------|
| YouTube | [![Tutorial](http://img.youtube.com/vi/u7og6yAx91g/0.jpg)](https://www.youtube.com/watch?v=u7og6yAx91g) |
| å“”å“©å“”å“© | [![è®­ç»ƒè„šæœ¬æ•™ç¨‹](https://i0.hdslb.com/bfs/archive/placeholder.jpg)](https://www.bilibili.com/video/BV1zaYnzoEoD/) |

## ï¿½ Dataset Information

**Processed Vietnamese Dataset:**
- **Total samples**: 38,625 Vietnamese TTS samples
- **Duration**: 42.4 hours of high-quality audio
- **Speakers**: 91 different Vietnamese speakers (balanced at 500 samples each)
- **Language**: Vietnamese with emotion detection (questioning, affirming, alerting, neutral)
- **Quality**: All samples validated and ready for training
- **Format**: Higgs Audio compatible (WAV + TXT + metadata.json)

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone https://github.com/thucth-qt/train-higgs-audio-vi.git
cd train-higgs-audio-vi

# CRITICAL: Set up memory optimization environment
bash setup_environment.sh

# Setup training environment
bash setup_training_env.sh
```

**âš ï¸ IMPORTANT**: The `setup_environment.sh` step is **mandatory** as it sets:
- `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`
- Other critical memory optimizations
- **Training will fail without these settings**

### 2. Data Processing (Already Completed)

The Vietnamese dataset has been processed and is ready for training:

```bash
# Data is located at:
# /home/thuc/thuc/voice/train-higgs-audio-vi/vietnamese_training_data_fast/
```

### 3. Start Training

```bash
# One-click training with optimal settings
bash train_vietnamese.sh

# Or manually with custom parameters
python trainer/trainer.py \
    --train_data_dir /home/thuc/thuc/voice/train-higgs-audio-vi/vietnamese_training_data_fast \
    --output_dir ./output/vietnamese_higgs_model \
    --use_lora --fp16
```

### 4. Monitor Training

```bash
# View training progress with TensorBoard
tensorboard --logdir ./logs/vietnamese_training
```

## ğŸ”§ Training Environment Setup

### Option 1: Using conda (Recommended)
```bash
git clone https://github.com/thucth-qt/train-higgs-audio-vi.git
cd train-higgs-audio-vi

conda create -n higgs_audio_env python=3.10
conda activate higgs_audio_env
pip install -r requirements_train.txt
pip install -e .
```

### Option 2: Using existing environment
```bash
# Setup training environment with required dependencies
bash setup_training_env.sh
```

## ğŸ“Š Vietnamese Data Processing

This repository includes enhanced data processing tools optimized for Vietnamese datasets.

### Enhanced Vietnamese Data Processor

The `data_preprocess_vn.py` script supports both CSV and Parquet formats with multithreading for optimal performance:

```bash
# For CSV-based datasets (like balanced Vietnamese dataset)
python tools/data_preprocess_vn.py --mode csv \
  --input /root/data/higgs/balanced_tts_dataset/balanced_metadata_full.csv \
  --audio_dir /root/data/higgs/balanced_tts_dataset/wavs/ \
  --output ./vietnamese_training_data \
  --max_workers 16 \
  --max_samples_per_speaker 10000

# For Parquet-based datasets (like VLSP2020)
python tools/data_preprocess_vn.py --mode parquet \
  --input /path/to/parquet/files \
  --output ./vietnamese_training_data \
  --dataset_name vn
```

### Jupyter Notebook Processing

For interactive data processing, use the provided notebook:

```bash
# Open the data preparation notebook
jupyter notebook exps/1-prepare-data.ipynb
```

The notebook provides:
- **Data validation and filtering**
- **Vietnamese emotion detection**
- **Speaker balancing**
- **Train/validation split creation**
- **Quality assurance checks**

### Data Format Requirements

#### Input CSV Format
Your CSV should contain these columns:
- `wav_filename`: Audio file name
- `text`: Original transcription
- `normalized_text`: Cleaned transcription (optional)
- `speaker`: Speaker identifier
- `duration`: Audio duration in seconds
- `valid`: Boolean indicating if sample is valid
- `gender`: Speaker gender
- `raw_path`: Full path to audio file

#### Output Higgs Audio Format

```shell
vietnamese_training_data/
â”œâ”€â”€ metadata.json                  # Overall metadata file of the dataset
â”œâ”€â”€ speaker1_000000.wav           # Audio file 1 of speaker
â”œâ”€â”€ speaker1_000000.txt           # Text transcription corresponding to the audio
â”œâ”€â”€ speaker1_000001.wav           # Audio file 2 of speaker
â”œâ”€â”€ speaker1_000001.txt           # Text transcription corresponding to the audio
â”œâ”€â”€ speaker2_000000.wav           # Audio file from different speaker
â”œâ”€â”€ speaker2_000000.txt           # Text transcription
â””â”€â”€ ...                           # More audio/text files
```

#### metadata.json Structure
```json
{
  "dataset_info": {
    "total_samples": 38625,
    "speakers": ["speaker1", "speaker2", "..."],
    "languages": ["vi"],
    "total_duration": 152640.5,
    "avg_duration": 3.95,
    "created_from": ["/path/to/source/dataset.csv"],
    "dataset_type": "vietnamese_tts",
    "sample_rate": 24000,
    "speaker_counts": {"speaker1": 500, "speaker2": 500, "...": "..."}
  },
  "samples": [
    {
      "id": "speaker1_000000",
      "audio_file": "speaker1_000000.wav",
      "transcript_file": "speaker1_000000.txt",
      "duration": 3.86,
      "speaker_id": "speaker1",
      "speaker_name": "Speaker 1",
      "scene": "quiet_room",
      "emotion": "neutral",
      "language": "vi",
      "gender": "unknown",
      "quality_score": 1.0,
      "original_audio_path": "/path/to/original/audio.wav",
      "user_instruction": "<audio>",
      "task_type": "audio_generation",
      "topic": "",
      "dialect": "",
      "sample_rate": 24000
    ```
train-higgs-audio-vi/
â”œâ”€â”€ README.md                             # This comprehensive guide
â”œâ”€â”€ TRAINING_GUIDE.md                     # Detailed training instructions
â”œâ”€â”€ requirements_train.txt                # Training dependencies
â”œâ”€â”€ setup_environment.sh                  # CRITICAL: CUDA memory setup
â”œâ”€â”€ setup_training_env.sh                 # Environment setup script
â”œâ”€â”€ train_vietnamese.sh                   # One-click training script (24GB+ GPU)
â”œâ”€â”€ train_vietnamese_emergency.sh         # Emergency low-memory training (20GB GPU)
â”œâ”€â”€ train_vietnamese_cpu.sh               # CPU training fallback
â”œâ”€â”€ setup_vietnamese_training.py          # Training configuration setup
â”œâ”€â”€ vietnamese_train.py                   # Generated training script
â”œâ”€â”€ memory_analysis.py                    # GPU memory analysis tool
â”‚
â”œâ”€â”€ exps/                                 # Experiments and notebooks
â”‚   â””â”€â”€ 1-prepare-data.ipynb             # Interactive data processing
â”‚
â”œâ”€â”€ tools/                                # Data processing utilities
â”‚   â”œâ”€â”€ data_preprocess_vn.py            # Enhanced Vietnamese data processor
â”‚   â”œâ”€â”€ convert_jsonl_to_dataset.py      # JSONL to Higgs format converter
â”‚   â”œâ”€â”€ test_audio_data.py               # Audio validation tools
â”‚   â””â”€â”€ validate_dataset.py              # Dataset validation
â”‚
â”œâ”€â”€ trainer/                              # Training pipeline
â”‚   â”œâ”€â”€ trainer.py                       # Main training script
â”‚   â”œâ”€â”€ trainer_ddp.py                   # Distributed training
â”‚   â””â”€â”€ README.md                        # Training documentation
â”‚
â”œâ”€â”€ vietnamese_training_data_fast/        # Processed Vietnamese dataset
â”‚   â”œâ”€â”€ metadata.json                    # Dataset metadata
â”‚   â”œâ”€â”€ speaker1_000000.wav             # Audio files
â”‚   â”œâ”€â”€ speaker1_000000.txt             # Transcriptions
â”‚   â””â”€â”€ ...                             # More samples
â”‚
â”œâ”€â”€ output/                               # Training outputs
â”‚   â””â”€â”€ vietnamese_higgs_model/          # Trained model checkpoints
â”‚
â””â”€â”€ logs/                                 # Training logs
    â””â”€â”€ vietnamese_training/             # TensorBoard logs
```

## ğŸš¨ Troubleshooting

### âš ï¸ GPU Memory Issues (COMMON)

**Problem**: "CUDA out of memory" with 16-20GB GPUs

**Root Cause**: Original documentation claims were incorrect. Analysis shows:
- Higgs Audio v2 documentation states: **"24GB for optimal performance"**
- Real memory usage: 18-22GB for LoRA training
- Parameter count: 5.2B (3B LLM + 2.2B audio tokenizer)

**Solutions by GPU Memory:**

#### 24GB+ GPUs (RTX 4090, V100, A100) âœ…
```bash
# Standard training works
bash train_vietnamese.sh
```

#### 20GB GPUs (RTX A4500/A5000) âš ï¸ 
```bash
# Try emergency ultra-low memory mode
bash train_vietnamese_emergency.sh

# Parameters used:
# - LoRA rank: 2 (minimal)
# - Batch size: 1
# - Gradient accumulation: 16
# - Sequence length: 1024
# - Freeze most model components
```

#### 16GB GPUs (RTX 4060 Ti, RTX 3080) âŒ
```bash
# Only CPU training works (very slow)
bash train_vietnamese_cpu.sh

# Estimated time: 24-48 hours
```

#### Cloud GPU Alternatives
- **Google Colab Pro**: A100 40GB ($10/month)
- **AWS EC2**: g5.xlarge with A10G 24GB
- **RunPod**: RTX 4090 24GB (~$0.50/hour)
- **Vast.ai**: Various GPUs, competitive pricing

### âš¡ Critical Memory Settings

**IMPORTANT**: Always set this environment variable before training:

```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

**Why this is critical:**
- Prevents CUDA memory fragmentation
- Allows dynamic memory expansion
- Required for models with 18+ GB memory usage
- **Training will likely fail without this setting**

**Advanced memory optimization:**
```bash
# For 20GB GPUs (more aggressive)
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128

# For emergency low memory (most aggressive)
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:64
```

**In Python scripts:**
```python
import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
import torch  # Import after setting env var
```

### Common Issues and Solutions

#### GPU Memory Issues
```bash
# CRITICAL: Always set this environment variable before training
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# Reduce batch size
--per_device_train_batch_size 1

# Enable gradient checkpointing
--gradient_checkpointing

# Use FP16 mixed precision
--fp16
```

#### CUDA Compatibility
```bash
# Check CUDA version
nvidia-smi

# Install compatible PyTorch
pip install torch==2.0.0+cu118 torchaudio==2.0.0+cu118 --index-url https://download.pytorch.org/whl/cu118
```

#### Data Loading Issues
```bash
# Validate dataset format
python tools/validate_dataset.py --data_dir ./vietnamese_training_data_fast

# Check file permissions
chmod -R 755 ./vietnamese_training_data_fast
```

#### Training Speed Optimization
```bash
# Increase dataloader workers
--dataloader_num_workers 8

# Use faster data format
# Ensure audio files are in WAV format with 24kHz sample rate
```

### Performance Benchmarks

**âš ï¸ IMPORTANT: Memory Requirements Corrected**

The original claims of "16GB sufficient" were **misleading**. Real-world testing shows:

| Configuration | GPU | **Actual Memory Usage** | Training Speed | Time per Epoch | Status |
|---------------|-----|-------------------------|----------------|----------------|---------|
| LoRA + FP16 | RTX 4090 (24GB) | ~18-20GB | 2.8 samples/sec | ~4 hours | âœ… Works |
| LoRA + FP16 | RTX A4500 (20GB) | **18+ GB (OOM)** | - | - | âŒ Fails |
| LoRA + FP16 | V100 32GB | ~22-26GB | 2.1 samples/sec | ~5.5 hours | âœ… Works |
| Full + FP16 | A100 40GB | ~35GB | 1.2 samples/sec | ~9 hours | âœ… Works |

**ğŸ“Š Memory Analysis Results:**
- **Model parameters**: 5.2B total (3B LLM + 2.2B audio)
- **Parameter memory**: ~10.4GB (bfloat16)
- **Training overhead**: ~8-12GB (gradients, optimizer, activations)
- **Total LoRA training**: **18-22GB minimum**
- **Inference only**: ~12GB

## ğŸ¯ Training Results

### Expected Performance
After 5 epochs of training on the Vietnamese dataset, you can expect:

- **Voice Quality**: High-quality Vietnamese speech synthesis
- **Speaker Variety**: Support for 91 different Vietnamese speakers
- **Emotion Control**: Natural emotional expression in generated speech
- **Language Accuracy**: Proper Vietnamese pronunciation and intonation

### Model Outputs
- **Base Model**: Fine-tuned Higgs Audio v2 for Vietnamese
- **LoRA Adapters**: Lightweight adaptation layers (if using LoRA)
- **Checkpoints**: Intermediate models saved every 1000 steps
- **Logs**: Comprehensive training metrics and progress

## ğŸ”§ Additional Tools

### Data Validation
```bash
# Check your GPU memory compatibility
python memory_analysis.py

# Validate processed dataset
python tools/validate_dataset.py --data_dir ./vietnamese_training_data_fast

# Test audio quality
python tools/test_audio_data.py --data_dir ./vietnamese_training_data_fast
```

### Model Merging (LoRA)
```bash
# Merge LoRA adapters with base model
bash merge_model.sh \
    --base_model_path ./output/vietnamese_higgs_model \
    --lora_path ./output/vietnamese_higgs_model/lora_adapters \
    --output_path ./output/vietnamese_higgs_merged
```

### Generation Testing
```bash
# Test the trained model
bash generate.sh \
    --model_path ./output/vietnamese_higgs_model \
    --text "Xin chÃ o, tÃ´i lÃ  trá»£ lÃ½ AI báº±ng tiáº¿ng Viá»‡t" \
    --output_path ./test_output.wav
```

## ğŸ“š Documentation

- **[TRAINING_GUIDE.md](TRAINING_GUIDE.md)**: Comprehensive training instructions
- **[trainer/README.md](trainer/README.md)**: Detailed training script documentation
- **[Jupyter Notebooks](exps/)**: Interactive data processing and analysis

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/vietnamese-improvements`)
3. Commit your changes (`git commit -am 'Add Vietnamese language improvements'`)
4. Push to the branch (`git push origin feature/vietnamese-improvements`)
5. Create a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Boson AI**: For the original Higgs Audio v2 architecture
- **Vietnamese TTS Community**: For dataset contributions and feedback
- **Hugging Face**: For model hosting and distribution
- **Contributors**: Everyone who helped improve this Vietnamese TTS training pipeline

## ğŸ“ Support

For questions and support:
- **Issues**: [GitHub Issues](https://github.com/thucth-qt/train-higgs-audio-vi/issues)
- **Discussions**: [GitHub Discussions](https://github.com/thucth-qt/train-higgs-audio-vi/discussions)
- **Documentation**: Check the `TRAINING_GUIDE.md` for detailed instructions

---

<div align="center">
<b>ğŸ‡»ğŸ‡³ Vietnamese TTS Training with Higgs Audio v2 ğŸ™ï¸</b><br>
<i>High-quality Vietnamese Text-to-Speech model training made easy</i>
</div>
```

## ğŸ‹ï¸ Training Configuration

### Model Architecture
- **Base Model**: `bosonai/higgs-audio-v2-generation-3B-base`
- **Audio Tokenizer**: `bosonai/higgs-audio-v2-tokenizer`
- **Task Type**: Vietnamese TTS (single_speaker_smart_voice)

### Training Parameters
```yaml
# Core Training Settings
num_train_epochs: 5
per_device_train_batch_size: 2
learning_rate: 1e-4
warmup_steps: 1000

# LoRA Configuration (Memory Efficient)
use_lora: true
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1

# System Optimization
fp16: true                    # Mixed precision training
gradient_checkpointing: true  # Memory optimization
dataloader_num_workers: 4     # Parallel data loading

# Monitoring
logging_steps: 10
save_steps: 1000
eval_steps: 500
```

### Hardware Requirements
- **GPU**: CUDA-capable GPU with **24GB+ VRAM** (RTX 4090, V100, A100, RTX 6000)
  - âš ï¸ **20GB GPUs (RTX A4500/A5000) are insufficient for LoRA training**
  - 16GB GPUs can only run inference, not training
- **RAM**: 32GB+ system memory recommended
- **Storage**: ~50GB for model checkpoints and logs
- **Training Time**: ~20-30 hours for 5 epochs

## ğŸ”§ Training Scripts

### 1. Automated Training Script
```bash
# One-click training with optimal settings
bash train_vietnamese.sh
```

### 2. Manual Training
```bash
python trainer/trainer.py \
    --model_path "bosonai/higgs-audio-v2-generation-3B-base" \
    --audio_tokenizer_path "bosonai/higgs-audio-v2-tokenizer" \
    --train_data_dir "/path/to/vietnamese_training_data" \
    --output_dir "./output/vietnamese_higgs_model" \
    --task_type "single_speaker_smart_voice" \
    --num_train_epochs 5 \
    --per_device_train_batch_size 2 \
    --learning_rate 1e-4 \
    --use_lora \
    --fp16
```

### 3. Custom Configuration
```bash
# Setup and validate your configuration
python setup_vietnamese_training.py

# Generated training script
python vietnamese_train.py
```

## ğŸ“ˆ Training Monitoring

### TensorBoard Integration
```bash
# Start TensorBoard to monitor training
tensorboard --logdir ./logs/vietnamese_training

# Available metrics:
# - Training/validation loss
# - Learning rate schedule
# - GPU utilization
# - Training speed (samples/sec)
```

### Training Checkpoints
- **Auto-save**: Every 1000 steps (~45 minutes)
- **Best model**: Automatically saved based on validation loss
- **Location**: `./output/vietnamese_higgs_model/`
- **LoRA adapters**: Saved separately in `./output/vietnamese_higgs_model/lora_adapters/`

## ğŸ› ï¸ Advanced Features

### Vietnamese Language Optimizations
- **Emotion Detection**: Automatic classification (questioning, affirming, alerting, neutral)
- **Scene Detection**: Context-aware scene classification (quiet_room, studio, outdoor)
- **Multi-speaker Support**: Balanced training across 91 Vietnamese speakers
- **Text Normalization**: Vietnamese-specific text preprocessing

### Performance Optimizations
- **Multithreaded Processing**: 70% faster data preprocessing
- **Memory Efficient**: LoRA training reduces memory usage by ~60%
- **GPU Optimization**: FP16 mixed precision training
- **Batch Processing**: Optimized batch sizes for Vietnamese dataset

## ğŸ“ Repository Structure


## Training  è®­ç»ƒ

Please make sure to modify all parameters before training, including data path, model path, number of training epochs, etc.  
è¯·åŠ¡å¿…åœ¨è®­ç»ƒå‰ä¿®æ”¹å„ä¸ªå‚æ•°ï¼ŒåŒ…æ‹¬æ•°æ®è·¯å¾„ã€æ¨¡å‹è·¯å¾„ã€è®­ç»ƒè½®æ•°ç­‰ã€‚

```shell
python trainer/trainer.py
```

Fine-tuning with LoRA requires the use of `--use_lora`, like:

```shell
python trainer/trainer.py --use_lora
```

It should be noted that when using LoRA to fine-tune new voices, there may be cases where normal output cannot be achieved. This issue has currently been found in the migration fine-tuning of Vietnamese, and it is not yet clear whether it is a training problem or other circumstances. Based on past experience, when training a model to learn knowledge it has never been exposed to, it is better to use full fine-tuning with the parameter `--use_lora False`.





## Merge lora
```shell
bash merge_model.sh \
    --base_model_path xxx \
    --lora_adapter_path xxx \
    --output_path xxx \
    --compare_models \
    --test_input "A custom sentence for testing." 
```

## generate  ç”Ÿæˆ

```shell
bash generate.sh
```

## Experiment Comparison: Text and Audio Effect Comparison  å®éªŒå¯¹æ¯”ï¼šæ–‡æœ¬ä¸éŸ³é¢‘æ•ˆæœå¯¹ç…§

To intuitively show the difference between generated sounds and real sounds, the following table contains directly playable audio files:  
ä¸ºç›´è§‚å±•ç¤ºç”Ÿæˆå£°éŸ³ä¸çœŸå®å£°éŸ³çš„å·®å¼‚ï¼Œä»¥ä¸‹è¡¨æ ¼åŒ…å«å¯ç›´æ¥æ’­æ”¾çš„éŸ³é¢‘æ–‡ä»¶ï¼š

Since the data I have is the speech of hearing-impaired individuals, for the purpose of comparison, I selected a speech sample from a hearing-impaired person as the real voice, and a generated version of the same speech as the generated voice.
å› ä¸ºæˆ‘æ‰‹ä¸Šçš„æ•°æ®æ˜¯å¬éšœäººå£«çš„è¯­éŸ³ï¼Œå› æ­¤åœ¨å¯¹æ¯”æ—¶ï¼Œæˆ‘é€‰æ‹©äº†ä¸€ä¸ªå¬éšœäººå£«çš„è¯­éŸ³ä½œä¸ºçœŸå®å£°éŸ³ï¼Œå¦ä¸€ä¸ªç›¸åŒè¯­éŸ³çš„ç”Ÿæˆç‰ˆæœ¬ä½œä¸ºç”Ÿæˆå£°éŸ³ã€‚



| text æ–‡æœ¬å†…å®¹ | real record çœŸå®å£°éŸ³ï¼ˆç”¨æˆ·åå½•ï¼‰ | generate recordç”Ÿæˆå£°éŸ³ï¼ˆè„šæœ¬è¾“å‡ºï¼‰ |
|----------|----------------------|----------------------|
| å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ç«å›ï¼Œæˆ‘å±…ä½åœ¨ä¸Šæµ· | [ç‚¹å‡»æ’­æ”¾/ä¸‹è½½ (huojun.MP3)](test_demo/huojun.MP3) | [ç‚¹å‡»æ’­æ”¾/ä¸‹è½½ (huojun_gen.wav)](test_demo/huojun_gen.wav) |
| æˆ‘çˆ±æœºæ™ºæµï¼Œæœºæ™ºæµæ˜¯æœ€å¥½çš„å¼€æºç¤¾åŒº | [ç‚¹å‡»æ’­æ”¾/ä¸‹è½½ (smartflowai.MP3)](test_demo/smartflowai.MP3) | [ç‚¹å‡»æ’­æ”¾/ä¸‹è½½ (smartflowai_gen.wav)](test_demo/smartflowai_gen.wav) |
| tÃ´i cÅ©ng nhÆ° lÃ  nhá»¯ng ngÆ°á»i lÃ­nh nhÆ° | [ç‚¹å‡»æ’­æ”¾/ä¸‹è½½ (vn_demo.MP3)](test_demo/vn_demo.MP3) | [ç‚¹å‡»æ’­æ”¾/ä¸‹è½½ (vn_gen.wav)](test_demo/vn_gen.wav) |

è®­ç»ƒå‰åå¯¹æ¯”(æ²¡æœ‰ä½¿ç”¨å‚è€ƒéŸ³é¢‘)
| text æ–‡æœ¬å†…å®¹ | before training è®­ç»ƒå‰ | after training è®­ç»ƒå |
|----------|----------------------|----------------------|
| ä½ å¥½ï¼Œæˆ‘æ˜¯ç«å› | [ç‚¹å‡»æ’­æ”¾/ä¸‹è½½ (huojun.MP3)](test_demo/generation_for_huo_no_ref_no_train.wav) | [ç‚¹å‡»æ’­æ”¾/ä¸‹è½½ (huojun_gen.wav)](test_demo/generation_for_huo_no_ref.wav) |

We are open-sourcing Higgs Audio v2, a powerful audio foundation model pretrained on over 10 million hours of audio data and a diverse set of text data. Despite having no post-training or fine-huoing, Higgs Audio v2 excels in expressive audio generation, thanks to its deep language and acoustic understanding.

On [EmergentTTS-Eval](https://github.com/boson-ai/emergenttts-eval-public), it achieves win rates of **75.7%** and **55.7%** over "gpt-4o-mini-tts" on the "Emotions" and "Questions" categories, respectively. It also obtains state-of-the-art performance on traditional TTS benchmarks like Seed-TTS Eval and Emotional Speech Dataset (ESD). Moreover, the model demonstrates capabilities rarely seen in previous systems, including generating natural multi-speaker dialogues in multiple languages, automatic prosody adaptation during narration, melodic humming with the cloned voice, and simultaneous generation of speech and background music.

<p align="center">
    <img src="figures/emergent-tts-emotions-win-rate.png" width=900>
</p>

Here's the demo video that shows some of its emergent capabilities (remember to unmute):

<video src="https://github.com/user-attachments/assets/0fd73fad-097f-48a9-9f3f-bc2a63b3818d" type="video/mp4" width="80%" controls>
</video>

Here's another demo video that show-cases the model's multilingual capability and how it enabled live translation (remember to unmute):

<video src="https://github.com/user-attachments/assets/2b9b01ff-67fc-4bd9-9714-7c7df09e38d6" type="video/mp4" width="80%" controls>
</video>

## Installation

We recommend to use NVIDIA Deep Learning Container to manage the CUDA environment. Following are two docker images that we have verified:
- nvcr.io/nvidia/pytorch:25.02-py3
- nvcr.io/nvidia/pytorch:25.01-py3

Here's an example command for launching a docker container environment. Please also check the [official NVIDIA documentations](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch).

```bash
docker run --gpus all --ipc=host --net=host --ulimit memlock=-1 --ulimit stack=67108864 -it --rm nvcr.io/nvidia/pytorch:25.02-py3 bash
```

### Option 1: Direct installation


```bash
git clone https://github.com/boson-ai/higgs-audio.git
cd higgs-audio

pip install -r requirements.txt
pip install -e .
```

### Option 2: Using venv

```bash
git clone https://github.com/boson-ai/higgs-audio.git
cd higgs-audio

python3 -m venv higgs_audio_env
source higgs_audio_env/bin/activate
pip install -r requirements.txt
pip install -e .
```


### Option 3: Using conda
```bash
git clone https://github.com/boson-ai/higgs-audio.git
cd higgs-audio

conda create -n higgs_audio_env python=3.10
conda activate higgs_audio_env
pip install -r requirements.txt
pip install -e .
```

### Option 4: Using uv
```bash
git clone https://github.com/boson-ai/higgs-audio.git
cd higgs-audio

uv venv --python 3.10
source .venv/bin/activate
uv pip install -r requirements.txt
uv pip install -e .
```

### Option 5: Using vllm

For advanced usage with higher throughput, we also built OpenAI compatible API server backed by vLLM engine for you to use.
Please refer to [examples/vllm](./examples/vllm) for more details.


## Usage

> [!TIP]
> For optimal performance, run the generation examples on a machine equipped with GPU with at least 24GB memory!

### Get Started

Here's a basic python snippet to help you get started.

```python
from boson_multimodal.serve.serve_engine import HiggsAudioServeEngine, HiggsAudioResponse
from boson_multimodal.data_types import ChatMLSample, Message, AudioContent

import torch
import torchaudio
import time
import click

MODEL_PATH = "bosonai/higgs-audio-v2-generation-3B-base"
AUDIO_TOKENIZER_PATH = "bosonai/higgs-audio-v2-tokenizer"

system_prompt = (
    "Generate audio following instruction.\n\n<|scene_desc_start|>\nAudio is recorded from a quiet room.\n<|scene_desc_end|>"
)

messages = [
    Message(
        role="system",
        content=system_prompt,
    ),
    Message(
        role="user",
        content="The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.",
    ),
]
device = "cuda" if torch.cuda.is_available() else "cpu"

serve_engine = HiggsAudioServeEngine(MODEL_PATH, AUDIO_TOKENIZER_PATH, device=device)

output: HiggsAudioResponse = serve_engine.generate(
    chat_ml_sample=ChatMLSample(messages=messages),
    max_new_tokens=1024,
    temperature=0.3,
    top_p=0.95,
    top_k=50,
    stop_strings=["<|end_of_text|>", "<|eot_id|>"],
)
torchaudio.save(f"output.wav", torch.from_numpy(output.audio)[None, :], output.sampling_rate)
```

We also provide a list of examples under [examples](./examples). In the following we highlight a few examples to help you use Higgs Audio v2.

### Zero-Shot Voice Cloning
Generate audio that sounds similar as the provided [reference audio](./examples/voice_prompts/belinda.wav).

```bash
python3 examples/generation.py \
--transcript "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years." \
--ref_audio belinda \
--temperature 0.3 \
--out_path generation.wav
```

The generation script will automatically use `cuda:0` if it founds cuda is available. To change the device id, specify `--device_id`:

```bash
python3 examples/generation.py \
--transcript "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years." \
--ref_audio belinda \
--temperature 0.3 \
--device_id 0 \
--out_path generation.wav
```

You can also try other voices. Check more example voices in [examples/voice_prompts](./examples/voice_prompts). You can also add your own voice to the folder.

```bash
python3 examples/generation.py \
--transcript "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years." \
--ref_audio broom_salesman \
--temperature 0.3 \
--out_path generation.wav
```

### Single-speaker Generation with Smart Voice
If you do not specify reference voice, the model will decide the voice based on the transcript it sees.

```bash
python3 examples/generation.py \
--transcript "The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years." \
--temperature 0.3 \
--out_path generation.wav
```


### Multi-speaker Dialog with Smart Voice
Generate multi-speaker dialog. The model will decide the voices based on the transcript it sees.

```bash
python3 examples/generation.py \
--transcript examples/transcript/multi_speaker/en_argument.txt \
--seed 12345 \
--out_path generation.wav
```

### Multi-speaker Dialog with Voice Clone

Generate multi-speaker dialog with the voices you picked.

```bash
python3 examples/generation.py \
--transcript examples/transcript/multi_speaker/en_argument.txt \
--ref_audio belinda,broom_salesman \
--ref_audio_in_system_message \
--chunk_method speaker \
--seed 12345 \
--out_path generation.wav
```


## Technical Details
<img src="figures/higgs_audio_v2_architecture_combined.png" width=900>


Higgs Audio v2 adopts the "generation variant" depicted in the architecture figure above. Its strong performance is driven by three key technical innovations:
- We developed an automated annotation pipeline that leverages multiple ASR models, sound event classification models, and our in-house audio understanding model. Using this pipeline, we cleaned and annotated 10 million hours audio data, which we refer to as **AudioVerse**. The in-house understanding model is finehuoed on top of [Higgs Audio v1 Understanding](https://www.boson.ai/blog/higgs-audio), which adopts the "understanding variant" shown in the architecture figure.
- We trained a unified audio tokenizer from scratch that captures both semantic and acoustic features. Learn more in the [tokenizer blog](./tech_blogs/TOKENIZER_BLOG.md).
- We proposed the DualFFN architecture, which enhances the LLMâ€™s ability to model acoustics tokens with minimal computational overhead. See the [architecture blog](./tech_blogs/ARCHITECTURE_BLOG.md).

## Evaluation

Here's the performance of Higgs Audio v2 on four benchmarks,  [Seed-TTS Eval](https://github.com/BytedanceSpeech/seed-tts-eval), [Emotional Speech Dataset (ESD)](https://paperswithcode.com/dataset/esd), [EmergentTTS-Eval](https://arxiv.org/abs/2505.23009), and Multi-speaker Eval:

#### Seed-TTS Eval & ESD

We prompt Higgs Audio v2 with the reference text, reference audio, and target text for zero-shot TTS. We use the standard evaluation metrics from Seed-TTS Eval and ESD.

|                              | SeedTTS-Eval| | ESD   |                 |
|------------------------------|--------|--------|---------|-------------------|
|                              | WER â†“ | SIM â†‘ | WER â†“ | SIM (emo2vec) â†‘ |
| Cosyvoice2                   | 2.28   | 65.49  | 2.71    | 80.48             |
| Qwen2.5-omniâ€                 | 2.33   | 64.10  | -       | -                 |
| ElevenLabs Multilingual V2   | **1.43**   | 50.00  | 1.66    | 65.87             |
| Higgs Audio v1                | 2.18   | 66.27  | **1.49**    | 82.84             |
| Higgs Audio v2 (base)         | 2.44   | **67.70**  | 1.78    | **86.13**         |


#### EmergentTTS-Eval ("Emotions" and "Questions")

Following the [EmergentTTS-Eval Paper](https://arxiv.org/abs/2505.23009), we report the win-rate over "gpt-4o-mini-tts" with the "alloy" voice. The judge model is Gemini 2.5 Pro.

| Model                              | Emotions (%) â†‘ | Questions (%) â†‘ |
|------------------------------------|--------------|----------------|
| Higgs Audio v2 (base)               | **75.71%**   | **55.71%**         |
| [gpt-4o-audio-previewâ€ ](https://platform.openai.com/docs/models/gpt-4o-audio-preview)       | 61.64%       | 47.85%         |
| [Hume.AI](https://www.hume.ai/research)                            | 61.60%       | 43.21%         |
| **BASELINE:** [gpt-4o-mini-tts](https://platform.openai.com/docs/models/gpt-4o-mini-tts)  | 50.00%       | 50.00%         |
| [Qwen 2.5 Omniâ€ ](https://github.com/QwenLM/Qwen2.5-Omni)      | 41.60%       | 51.78%         |
| [minimax/speech-02-hd](https://replicate.com/minimax/speech-02-hd)               | 40.86%        | 47.32%         |
| [ElevenLabs Multilingual v2](https://elevenlabs.io/blog/eleven-multilingual-v2)         | 30.35%       | 39.46%         |
| [DeepGram Aura-2](https://deepgram.com/learn/introducing-aura-2-enterprise-text-to-speech)                    | 29.28%       | 48.21%         |
| [Sesame csm-1B](https://github.com/SesameAILabs/csm)                      | 15.96%       | 31.78%         |

<sup><sub>'â€ ' means using the strong-prompting method described in the paper.</sub></sup>


#### Multi-speaker Eval

We also designed a multi-speaker evaluation benchmark to evaluate the capability of Higgs Audio v2 for multi-speaker dialog generation. The benchmark contains three subsets

- `two-speaker-conversation`: 1000 synthetic dialogues involving two speakers. We fix two reference audio clips to evaluate the model's ability in double voice cloning for utterances ranging from 4 to 10 dialogues between two randomly chosen persona.
- `small talk (no ref)`: 250 synthetic dialogues curated in the same way as above, but are characterized by short utterances and a limited number of turns (4â€“6), we do not fix reference audios in this case and this set is designed to evaluate the model's ability to automatically assign appropriate voices to speakers.
- `small talk (ref)`: 250 synthetic dialogues similar to above, but contains even shorter utterances as this set is meant to include reference clips in it's context, similar to `two-speaker-conversation`.


We report the word-error-rate (WER) and the geometric mean between intra-speaker similarity and inter-speaker dis-similarity on these three subsets. Other than Higgs Audio v2, we also evaluated [MoonCast](https://github.com/jzq2000/MoonCast) and [nari-labs/Dia-1.6B-0626](https://huggingface.co/nari-labs/Dia-1.6B-0626), two of the most popular open-source models capable of multi-speaker dialog generation. Results are summarized in the following table. We are not able to run [nari-labs/Dia-1.6B-0626](https://huggingface.co/nari-labs/Dia-1.6B-0626) on our "two-speaker-conversation" subset due to its strict limitation on the length of the utterances and output audio.

|                                                | two-speaker-conversation |                |small talk |                | small talk (no ref) |                |
| ---------------------------------------------- | -------------- | ------------------ | ---------- | -------------- | ------------------- | -------------- |
|                                                | WER â†“                      | Mean Sim & Dis-sim â†‘ | WER â†“       |  Mean Sim & Dis-sim â†‘ | WER â†“               | Mean Sim & Dis-sim â†‘ |
| [MoonCast](https://github.com/jzq2000/MoonCast) | 38.77                    | 46.02         | **8.33**       | 63.68          | 24.65               | 53.94 |
| [nari-labs/Dia-1.6B-0626](https://huggingface.co/nari-labs/Dia-1.6B-0626)         | \-                       | \-             | 17.62      | 63.15          | 19.46               | **61.14**          |
| Higgs Audio v2 (base)     | **18.88**                    | **51.95**          | 11.89      | **67.92**              | **14.65**               | 55.28              |


## Third-Party Licenses

The `boson_multimodal/audio_processing/` directory contains code derived from third-party repositories, primarily from [xcodec](https://github.com/zhenye234/xcodec). Please see the [`LICENSE`](boson_multimodal/audio_processing/LICENSE) in that directory for complete attribution and licensing information.
